name: Cursor AppImage Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: "0 2 * * *"

jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: [3.11, 3.12, 3.13]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            xvfb \
            x11-utils \
            xdotool \
            scrot \
            python3-tk \
            python3-dev \
            libxtst6 \
            libxss1 \
            libgconf-2-4 \
            libasound2 \
            libgtk-3-0 \
            libdrm2 \
            libxss1 \
            libgconf-2-4

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-html pytest-cov

      - name: Download Cursor AppImage (if not cached)
        run: |
          # Create a mock AppImage for testing in CI
          # TODO: In real usage, you would download or use the actual Cursor AppImage
          mkdir -p /tmp/cursor
          echo '#!/bin/bash' > /tmp/cursor/cursor.AppImage
          echo 'echo "Mock Cursor AppImage"' >> /tmp/cursor/cursor.AppImage
          echo 'sleep 30' >> /tmp/cursor/cursor.AppImage
          chmod +x /tmp/cursor/cursor.AppImage

          # Create config to use the mock AppImage
          mkdir -p config
          echo "CURSOR_PATH=/tmp/cursor/cursor.AppImage" > config/test.env

      - name: Create test directories
        run: |
          mkdir -p reports
          mkdir -p screenshots
          mkdir -p test_images

      - name: Run basic tests
        run: |
          xvfb-run -a --server-args="-screen 0 1920x1080x24 -ac +extension GLX" \
            python -m pytest tests/test_basic_functionality.py -v \
            --junitxml=reports/basic-junit.xml \
            --html=reports/basic-report.html
        env:
          DISPLAY: :99
          CURSOR_PATH: /tmp/cursor/cursor.AppImage

      - name: Run performance benchmarks
        run: |
          xvfb-run -a --server-args="-screen 0 1920x1080x24 -ac +extension GLX" \
            python -m pytest tests/test_performance_benchmarks.py -v \
            --junitxml=reports/performance-junit.xml \
            --html=reports/performance-report.html
        env:
          DISPLAY: :99
          CURSOR_PATH: /tmp/cursor/cursor.AppImage

      - name: Run workflow tests (if not slow)
        run: |
          xvfb-run -a --server-args="-screen 0 1920x1080x24 -ac +extension GLX" \
            python -m pytest tests/test_user_workflows.py -v \
            -m "not slow" \
            --junitxml=reports/workflow-junit.xml \
            --html=reports/workflow-report.html
        env:
          DISPLAY: :99
          CURSOR_PATH: /tmp/cursor/cursor.AppImage

      - name: Generate coverage report
        run: |
          pip install coverage
          coverage run -m pytest tests/ -m "not slow"
          coverage xml
          coverage html
        env:
          DISPLAY: :99
          CURSOR_PATH: /tmp/cursor/cursor.AppImage

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            reports/
            screenshots/
            htmlcov/
            *.json

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  benchmark:
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb x11-utils xdotool scrot
          pip install -r requirements.txt

      - name: Setup mock Cursor
        run: |
          mkdir -p /tmp/cursor
          echo '#!/bin/bash' > /tmp/cursor/cursor.AppImage
          echo 'echo "Mock Cursor AppImage"' >> /tmp/cursor/cursor.AppImage
          echo 'sleep 30' >> /tmp/cursor/cursor.AppImage
          chmod +x /tmp/cursor/cursor.AppImage

      - name: Run full benchmark suite
        run: |
          xvfb-run -a --server-args="-screen 0 1920x1080x24 -ac +extension GLX" \
            python -m pytest tests/test_performance_benchmarks.py -v
        env:
          DISPLAY: :99
          CURSOR_PATH: /tmp/cursor/cursor.AppImage

      - name: Store benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: |
            *.json
            screenshots/

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('benchmark_results.json')) {
              const results = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
              const comment = `## ðŸš€ Benchmark Results

              **Startup Time:** ${results.average.toFixed(2)}s (average)
              **Median:** ${results.median.toFixed(2)}s

              *Full results uploaded as artifacts*`;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
