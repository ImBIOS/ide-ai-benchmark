version: '3.8'

services:
  ide-ai-benchmark:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ide-ai-benchmark
    environment:
      # Display configuration
      - DISPLAY=:99
      - QT_X11_NO_MITSHM=1
      - XDG_RUNTIME_DIR=/tmp/runtime-testuser

      # IDE paths (mounted from host)
      - CURSOR_PATH=/app/ides/cursor.AppImage
      - WINDSURF_PATH=/app/ides/windsurf.AppImage
      - VSCODE_PATH=/usr/bin/code
      - TRAE_PATH=/app/ides/trae.AppImage

      # AI model API keys (set these in your .env file)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}

      # Test configuration
      - DEFAULT_IDE=${DEFAULT_IDE:-cursor}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-claude-3.5-sonnet}
      - TEST_TIMEOUT=${TEST_TIMEOUT:-300}

    volumes:
      # Mount IDE applications from host
      - ./ides:/app/ides:ro

      # Mount results and reports for persistence
      - ./results:/app/results
      - ./reports:/app/reports
      - ./screenshots:/app/screenshots
      - ./logs:/app/logs

      # Mount source code for development
      - ./src:/app/src
      - ./tests:/app/tests
      - ./scripts:/app/scripts

    # Use init to handle process management
    init: true

    # Allocate a TTY for better output formatting
    tty: true

    # Keep container running for interactive use
    stdin_open: true

    # Default command - override as needed
    command: ["python", "scripts/run_tests.py", "--quick"]

    # Health check
    healthcheck:
      test: ["CMD", "/app/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Service for running cross-IDE benchmarks
  cross-ide-benchmark:
    extends: ide-ai-benchmark
    container_name: cross-ide-benchmark
    command: ["python", "scripts/run_tests.py", "--cross-ide", "--all-ides"]

  # Service for performance benchmarking
  performance-benchmark:
    extends: ide-ai-benchmark
    container_name: performance-benchmark
    command: ["python", "scripts/run_tests.py", "--performance", "--all-ides"]

  # Service for AI model quality testing
  ai-quality-benchmark:
    extends: ide-ai-benchmark
    container_name: ai-quality-benchmark
    command: ["python", "scripts/run_tests.py", "--ai-models", "--all-ides"]

  # Service for comprehensive benchmarking
  comprehensive-benchmark:
    extends: ide-ai-benchmark
    container_name: comprehensive-benchmark
    command: ["python", "scripts/run_tests.py", "--comprehensive"]
    environment:
      - TEST_TIMEOUT=600  # Longer timeout for comprehensive tests

# Named volumes for persistent data
volumes:
  benchmark_results:
    driver: local
  benchmark_reports:
    driver: local

# Networks (none needed for this application)
networks:
  default:
    driver: bridge
